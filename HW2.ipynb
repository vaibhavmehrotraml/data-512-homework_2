{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55fb607e",
   "metadata": {},
   "source": [
    "# Wikipedia Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df349b53",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "127650ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import urllib.parse\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b72a4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file with names of all cities by states \n",
    "df_cities = pd.read_csv('us_cities_by_state_SEPT.2023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34e1eb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>page_title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abbeville,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Adamsville, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Adamsville,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Addison, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Addison,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Akron, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Akron,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Alabaster, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Alabaster,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21349</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>Waynesboro, Virginia</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Waynesboro,_Virg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21350</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>Williamsburg, Virginia</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Williamsburg,_Vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21351</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>Winchester, Virginia</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Winchester,_Virg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21865</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>County (United States)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/County_(United_S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22057</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>County (United States)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/County_(United_S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>638 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           state              page_title  \\\n",
       "461      Alabama      Abbeville, Alabama   \n",
       "462      Alabama     Adamsville, Alabama   \n",
       "463      Alabama        Addison, Alabama   \n",
       "464      Alabama          Akron, Alabama   \n",
       "465      Alabama      Alabaster, Alabama   \n",
       "...          ...                     ...   \n",
       "21349   Virginia    Waynesboro, Virginia   \n",
       "21350   Virginia  Williamsburg, Virginia   \n",
       "21351   Virginia    Winchester, Virginia   \n",
       "21865  Wisconsin  County (United States)   \n",
       "22057  Wisconsin  County (United States)   \n",
       "\n",
       "                                                     url  \n",
       "461     https://en.wikipedia.org/wiki/Abbeville,_Alabama  \n",
       "462    https://en.wikipedia.org/wiki/Adamsville,_Alabama  \n",
       "463       https://en.wikipedia.org/wiki/Addison,_Alabama  \n",
       "464         https://en.wikipedia.org/wiki/Akron,_Alabama  \n",
       "465     https://en.wikipedia.org/wiki/Alabaster,_Alabama  \n",
       "...                                                  ...  \n",
       "21349  https://en.wikipedia.org/wiki/Waynesboro,_Virg...  \n",
       "21350  https://en.wikipedia.org/wiki/Williamsburg,_Vi...  \n",
       "21351  https://en.wikipedia.org/wiki/Winchester,_Virg...  \n",
       "21865  https://en.wikipedia.org/wiki/County_(United_S...  \n",
       "22057  https://en.wikipedia.org/wiki/County_(United_S...  \n",
       "\n",
       "[638 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "df_cities[df_cities.page_title.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7149b5",
   "metadata": {},
   "source": [
    "> There are duplicate page_titles/urls, so let's drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14b01f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fet rid of duplicate state names or URLs\n",
    "df_cities = df_cities.drop_duplicates('url')\n",
    "df_cities = df_cities.drop_duplicates('page_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a2b1616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>page_title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abbeville,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Adamsville, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Adamsville,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Addison, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Addison,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Akron, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Akron,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Alabaster, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Alabaster,_Alabama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     state           page_title  \\\n",
       "0  Alabama   Abbeville, Alabama   \n",
       "1  Alabama  Adamsville, Alabama   \n",
       "2  Alabama     Addison, Alabama   \n",
       "3  Alabama       Akron, Alabama   \n",
       "4  Alabama   Alabaster, Alabama   \n",
       "\n",
       "                                                 url  \n",
       "0   https://en.wikipedia.org/wiki/Abbeville,_Alabama  \n",
       "1  https://en.wikipedia.org/wiki/Adamsville,_Alabama  \n",
       "2     https://en.wikipedia.org/wiki/Addison,_Alabama  \n",
       "3       https://en.wikipedia.org/wiki/Akron,_Alabama  \n",
       "4   https://en.wikipedia.org/wiki/Alabaster,_Alabama  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fea9e08",
   "metadata": {},
   "source": [
    "## Page Info Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fd796f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the template provided to request data from the API\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<vaibhav1@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2023',\n",
    "}\n",
    "\n",
    "ARTICLE_TITLES = ['Bison', 'Northern flicker', 'Red squirrel', 'Chinook salmon', 'Horseshoe bat']\n",
    "\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",           # to simplify this should be a single page title at a time\n",
    "    \"prop\": \"info\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae7efbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_pageinfo_per_article(article_title = None, \n",
    "                                 endpoint_url = API_ENWIKIPEDIA_ENDPOINT, \n",
    "                                 request_template = PAGEINFO_PARAMS_TEMPLATE,\n",
    "                                 headers = REQUEST_HEADERS):\n",
    "    \n",
    "\n",
    "    if article_title:\n",
    "        request_template['titles'] = article_title\n",
    "\n",
    "    if not request_template['titles']:\n",
    "        raise Exception(\"Must supply an article title to make a pageinfo request.\")\n",
    "\n",
    "    try:\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a32f437",
   "metadata": {},
   "source": [
    "### The API allows upto 50 articles to be requested at once, hence making the process faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98d09a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request and save data for future processing\n",
    "request_size = 50\n",
    "for i in tqdm(range(len(df_cities)//request_size+1)):\n",
    "    info = request_pageinfo_per_article(' | '.join(df_cities.page_title[i*request_size:i*request_size+request_size].tolist()))\n",
    "    with open(f'json_data/id_{i*request_size}_{i*request_size+request_size-1}.json', 'w') as f:\n",
    "        json.dump(info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d3d32ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read individual JSON files and combine into one large dataframe for processing\n",
    "df_list = []\n",
    "for file in os.listdir('json_data/'):    \n",
    "    with open(f'json_data/{file}') as f:\n",
    "        d = json.load(f)\n",
    "        df_list.append(pd.DataFrame(d['query']['pages']).T)\n",
    "\n",
    "df = pd.concat(df_list)\n",
    "df = df.sort_values(by='title')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631f4db8",
   "metadata": {},
   "source": [
    "### Merge the requested data with exsiting dataframe of city names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d882bf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge this newly donwloaded data with the dataset of all the cities\n",
    "df_merged = df[['pageid', 'title', 'lastrevid']].merge(df_cities, how='right', left_on='title', right_on='page_title').drop_duplicates('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ac53f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any city was missed by the API\n",
    "missing = set(df_cities.page_title).difference(set(df_merged.title))\n",
    "missing = list(missing)\n",
    "assert len(missing) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87f0d775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pageid</th>\n",
       "      <th>title</th>\n",
       "      <th>lastrevid</th>\n",
       "      <th>state</th>\n",
       "      <th>page_title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [pageid, title, lastrevid, state, page_title, url]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if any article in the merged dataset does not have a revision ID\n",
    "df_merged[df_merged.lastrevid.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6457adc",
   "metadata": {},
   "source": [
    "We can see that no articles are missing from our API calls and that each item in the merged dataframe has a lastrevid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90c6425",
   "metadata": {},
   "source": [
    "# ORES Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28290f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using our new dataset, query the ORES API to get article scores\n",
    "API_ORES_LIFTWING_ENDPOINT = \"https://api.wikimedia.org/service/lw/inference/v1/models/{model_name}:predict\"\n",
    "API_ORES_EN_QUALITY_MODEL = \"enwiki-articlequality\"\n",
    "\n",
    "\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (60.0/5000.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "REQUEST_HEADER_TEMPLATE = {\n",
    "    'User-Agent': \"<{email_address}>, University of Washington, MSDS DATA 512 - AUTUMN 2023\",\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': \"Bearer {access_token}\"\n",
    "}\n",
    "\n",
    "REQUEST_HEADER_PARAMS_TEMPLATE = {\n",
    "    'email_address' : \"vaibhav1@uw.edu\",         # your email address should go here\n",
    "    'access_token'  : \"\"          # the access token you create will need to go here\n",
    "}\n",
    "\n",
    "\n",
    "ARTICLE_REVISIONS = { 'Bison':1085687913 , 'Northern flicker':1086582504 , 'Red squirrel':1083787665 , 'Chinook salmon':1085406228 , 'Horseshoe bat':1060601936 }\n",
    "\n",
    "\n",
    "ORES_REQUEST_DATA_TEMPLATE = {\n",
    "    \"lang\":        \"en\",     # required that its english - we're scoring English Wikipedia revisions\n",
    "    \"rev_id\":      \"\",       # this request requires a revision id\n",
    "    \"features\":    True\n",
    "}\n",
    "\n",
    "# Personal Access Token, key has been removed to prevent misuse\n",
    "USERNAME = \"Vbrox2121\"\n",
    "ACCESS_TOKEN = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c759cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_ores_score_per_article(article_revid = None, email_address=None, access_token=None,\n",
    "                                   endpoint_url = API_ORES_LIFTWING_ENDPOINT, \n",
    "                                   model_name = API_ORES_EN_QUALITY_MODEL, \n",
    "                                   request_data = ORES_REQUEST_DATA_TEMPLATE, \n",
    "                                   header_format = REQUEST_HEADER_TEMPLATE, \n",
    "                                   header_params = REQUEST_HEADER_PARAMS_TEMPLATE):\n",
    "    \n",
    "    #    Make sure we have an article revision id, email and token\n",
    "    #    This approach prioritizes the parameters passed in when making the call\n",
    "    if article_revid:\n",
    "        request_data['rev_id'] = article_revid\n",
    "    if email_address:\n",
    "        header_params['email_address'] = email_address\n",
    "    if access_token:\n",
    "        header_params['access_token'] = access_token\n",
    "    \n",
    "    #   Making a request requires a revision id - an email address - and the access token\n",
    "    if not request_data['rev_id']:\n",
    "        raise Exception(\"Must provide an article revision id (rev_id) to score articles\")\n",
    "    if not header_params['email_address']:\n",
    "        raise Exception(\"Must provide an 'email_address' value\")\n",
    "    if not header_params['access_token']:\n",
    "        raise Exception(\"Must provide an 'access_token' value\")\n",
    "    \n",
    "    # Create the request URL with the specified model parameter - default is a article quality score request\n",
    "    request_url = endpoint_url.format(model_name=model_name)\n",
    "    \n",
    "    # Create a compliant request header from the template and the supplied parameters\n",
    "    headers = dict()\n",
    "    for key in header_format.keys():\n",
    "        headers[str(key)] = header_format[key].format(**header_params)\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free data\n",
    "        # source like ORES - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        #response = requests.get(request_url, headers=headers)\n",
    "        response = requests.post(request_url, headers=headers, data=json.dumps(request_data))\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0894398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and configure logger to keep track of each API request\n",
    "logging.basicConfig(filename=f\"pipeline.log\",\n",
    "                    format='%(asctime)s %(message)s',\n",
    "                    filemode='a',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Use TQDM to get a progress bar\n",
    "with tqdm(df_merged.lastrevid) as t:\n",
    "    for id in t:\n",
    "        score = request_ores_score_per_article(article_revid=id,\n",
    "                                           email_address=\"vaibhav1@uw.edu\",\n",
    "                                           access_token=ACCESS_TOKEN)\n",
    "        # Save each JSON dump into a file for downstream processing\n",
    "        with open(f'ores_json_data/{id}.json', 'w') as f:\n",
    "            json.dump(score, f)\n",
    "        \n",
    "        logger.info(f'Processed ID {id} with Score {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6095cd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON data and combine into one dataframe, focusing just on the scores\n",
    "df_list = []\n",
    "missing = []\n",
    "for file in os.listdir('ores_json_data/'):    \n",
    "    with open(f'ores_json_data/{file}') as f:\n",
    "        try:\n",
    "            d = json.load(f)\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f'Error {e} in {file}')\n",
    "        if 'httpReason' in d.keys():\n",
    "            missing.append(file)\n",
    "            print('No record found for', file)\n",
    "            continue\n",
    "        df_list.append(pd.DataFrame(d['enwiki']['scores']).T)\n",
    "\n",
    "df_ores = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c449fedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that no article was missed\n",
    "assert missing == []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b51a91",
   "metadata": {},
   "source": [
    "> Looks like we were able to get ORES scores for all articles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49fe3f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas magic to get one clean dataframe\n",
    "df_pred = df_ores.articlequality.apply(pd.Series).score.apply(pd.Series)\n",
    "df_pred = df_pred.reset_index()\n",
    "df_pred['index'] = df_pred['index'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb15b8af",
   "metadata": {},
   "source": [
    "### Merge ORES scores with our previous dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10b78f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the previous dataset with city names and the revision id of thier articles along with the ORES scores\n",
    "df_result = df_merged.merge(df_pred, left_on='lastrevid', right_on='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "669cb2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take subset of relevant fields to keep the data clean\n",
    "df_result = df_result[['pageid', 'title', 'lastrevid', 'state', 'page_title', 'prediction']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54d3631",
   "metadata": {},
   "source": [
    "## Read Census Data for regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0ef67d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the excel file which associates states to region and divions\n",
    "census_states = pd.read_excel('US States by Region - US Census Bureau.xlsx', index_col=[0,1])\n",
    "census_states = census_states.dropna()\n",
    "census_states = census_states.reset_index()\n",
    "census_states.STATE = census_states.STATE.str.replace(' ', '_') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0388639b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Georgia_(U.S._state)'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see that Georgia_(U.S._state) is present in our merged dataframe but not in the census dataset\n",
    "# This will be lost in the join, and is ignored for now\n",
    "(set(df_result.state.unique())).difference(set(census_states.STATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea346cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Georgia\n",
      "Nebraska\n",
      "Connecticut\n"
     ]
    }
   ],
   "source": [
    "# We can see that 'Connecticut', 'Georgia', 'Nebraska' present in the census data but not in our analysis\n",
    "# These will be lost in the join, and are ignored for now\n",
    "for state in (set(census_states.STATE)).difference((set(df_result.state))):\n",
    "    print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1643b326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the census data with our dataset\n",
    "df_result = df_result.merge(census_states, left_on='state', right_on='STATE').drop(['STATE'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccadea3",
   "metadata": {},
   "source": [
    "## Read Population Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a54a517b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Geographic Area</th>\n",
       "      <th>April 1, 2020 Estimates Base</th>\n",
       "      <th>Population Estimate (as of July 1)</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States</td>\n",
       "      <td>331449520.0</td>\n",
       "      <td>331511512.0</td>\n",
       "      <td>332031554.0</td>\n",
       "      <td>333287557.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Northeast</td>\n",
       "      <td>57609156.0</td>\n",
       "      <td>57448898.0</td>\n",
       "      <td>57259257.0</td>\n",
       "      <td>57040406.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Midwest</td>\n",
       "      <td>68985537.0</td>\n",
       "      <td>68961043.0</td>\n",
       "      <td>68836505.0</td>\n",
       "      <td>68787595.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>South</td>\n",
       "      <td>126266262.0</td>\n",
       "      <td>126450613.0</td>\n",
       "      <td>127346029.0</td>\n",
       "      <td>128716192.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>West</td>\n",
       "      <td>78588565.0</td>\n",
       "      <td>78650958.0</td>\n",
       "      <td>78589763.0</td>\n",
       "      <td>78743364.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.Alabama</td>\n",
       "      <td>5024356.0</td>\n",
       "      <td>5031362.0</td>\n",
       "      <td>5049846.0</td>\n",
       "      <td>5074296.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>.Alaska</td>\n",
       "      <td>733378.0</td>\n",
       "      <td>732923.0</td>\n",
       "      <td>734182.0</td>\n",
       "      <td>733583.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.Arizona</td>\n",
       "      <td>7151507.0</td>\n",
       "      <td>7179943.0</td>\n",
       "      <td>7264877.0</td>\n",
       "      <td>7359197.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>.Arkansas</td>\n",
       "      <td>3011555.0</td>\n",
       "      <td>3014195.0</td>\n",
       "      <td>3028122.0</td>\n",
       "      <td>3045637.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>.California</td>\n",
       "      <td>39538245.0</td>\n",
       "      <td>39501653.0</td>\n",
       "      <td>39142991.0</td>\n",
       "      <td>39029342.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Geographic Area  April 1, 2020 Estimates Base  \\\n",
       "1    United States                   331449520.0   \n",
       "2        Northeast                    57609156.0   \n",
       "3          Midwest                    68985537.0   \n",
       "4            South                   126266262.0   \n",
       "5             West                    78588565.0   \n",
       "6         .Alabama                     5024356.0   \n",
       "7          .Alaska                      733378.0   \n",
       "8         .Arizona                     7151507.0   \n",
       "9        .Arkansas                     3011555.0   \n",
       "10     .California                    39538245.0   \n",
       "\n",
       "    Population Estimate (as of July 1)   Unnamed: 3   Unnamed: 4  \n",
       "1                          331511512.0  332031554.0  333287557.0  \n",
       "2                           57448898.0   57259257.0   57040406.0  \n",
       "3                           68961043.0   68836505.0   68787595.0  \n",
       "4                          126450613.0  127346029.0  128716192.0  \n",
       "5                           78650958.0   78589763.0   78743364.0  \n",
       "6                            5031362.0    5049846.0    5074296.0  \n",
       "7                             732923.0     734182.0     733583.0  \n",
       "8                            7179943.0    7264877.0    7359197.0  \n",
       "9                            3014195.0    3028122.0    3045637.0  \n",
       "10                          39501653.0   39142991.0   39029342.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load population excel file downloaded from https://www.census.gov/data/tables/time-series/demo/popest/2020s-state-total.html\n",
    "df_pop = pd.read_excel('NST-EST2022-POP.xlsx', header=[2])[1:]\n",
    "df_pop.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ae376e",
   "metadata": {},
   "source": [
    "> This needs some cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4f7b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we are using jsons, ' ' character can be changed to '_' often\n",
    "df_pop['Geographic Area'] = df_pop['Geographic Area'].str.replace('.', '')\n",
    "df_pop['Geographic Area'] = df_pop['Geographic Area'].str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe872f42",
   "metadata": {},
   "source": [
    "> Due to some formatting issues, we can see that parts of the meta data are coming in as data cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1048e562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Geographic Area</th>\n",
       "      <th>April 1, 2020 Estimates Base</th>\n",
       "      <th>Population Estimate (as of July 1)</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Puerto_Rico</td>\n",
       "      <td>3285874.0</td>\n",
       "      <td>3281557.0</td>\n",
       "      <td>3262693.0</td>\n",
       "      <td>3221789.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Note:_The_estimates_are_developed_from_a_base_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Suggested_Citation:</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Annual_Estimates_of_the_Resident_Population_fo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Source:_US_Census_Bureau,_Population_Division</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Release_Date:_December_2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Geographic Area  \\\n",
       "58                                        Puerto_Rico   \n",
       "59  Note:_The_estimates_are_developed_from_a_base_...   \n",
       "60                                Suggested_Citation:   \n",
       "61  Annual_Estimates_of_the_Resident_Population_fo...   \n",
       "62      Source:_US_Census_Bureau,_Population_Division   \n",
       "63                        Release_Date:_December_2022   \n",
       "\n",
       "    April 1, 2020 Estimates Base  Population Estimate (as of July 1)  \\\n",
       "58                     3285874.0                           3281557.0   \n",
       "59                           NaN                                 NaN   \n",
       "60                           NaN                                 NaN   \n",
       "61                           NaN                                 NaN   \n",
       "62                           NaN                                 NaN   \n",
       "63                           NaN                                 NaN   \n",
       "\n",
       "    Unnamed: 3  Unnamed: 4  \n",
       "58   3262693.0   3221789.0  \n",
       "59         NaN         NaN  \n",
       "60         NaN         NaN  \n",
       "61         NaN         NaN  \n",
       "62         NaN         NaN  \n",
       "63         NaN         NaN  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the tail of the dataset, we can see which rows to delete\n",
    "df_pop.tail(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "813ccab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pop = df_pop[:58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a2a306e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Connecticut',\n",
       " 'District_of_Columbia',\n",
       " 'Georgia',\n",
       " 'Midwest',\n",
       " 'Nebraska',\n",
       " 'Northeast',\n",
       " 'Puerto_Rico',\n",
       " 'South',\n",
       " 'United_States',\n",
       " 'West',\n",
       " nan}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_pop['Geographic Area']).difference(set(df_result.state))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d851fb5b",
   "metadata": {},
   "source": [
    "> This is a list of region/states that are not included in out analysis but are provided in the population census. We have already noted that some of them were missing from our initial list, whereas others like Puerto Rico are new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ffbebf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df_result.merge(df_pop[['Geographic Area', 'April 1, 2020 Estimates Base']], left_on='state', right_on='Geographic Area').drop(['Geographic Area', 'page_title'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01c17aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result['regional_devision'] = df_result.apply(lambda x: f'{x.REGION} ({x.DIVISION})', axis=1)\n",
    "df_result = df_result.drop(['REGION', 'DIVISION'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28b4007b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pageid</th>\n",
       "      <th>title</th>\n",
       "      <th>lastrevid</th>\n",
       "      <th>state</th>\n",
       "      <th>prediction</th>\n",
       "      <th>April 1, 2020 Estimates Base</th>\n",
       "      <th>regional_devision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>104730</td>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>1171163550</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>C</td>\n",
       "      <td>5024356.0</td>\n",
       "      <td>South (East South Central)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104761</td>\n",
       "      <td>Adamsville, Alabama</td>\n",
       "      <td>1177621427</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>C</td>\n",
       "      <td>5024356.0</td>\n",
       "      <td>South (East South Central)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>105188</td>\n",
       "      <td>Addison, Alabama</td>\n",
       "      <td>1168359898</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>C</td>\n",
       "      <td>5024356.0</td>\n",
       "      <td>South (East South Central)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104726</td>\n",
       "      <td>Akron, Alabama</td>\n",
       "      <td>1165909508</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>GA</td>\n",
       "      <td>5024356.0</td>\n",
       "      <td>South (East South Central)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105109</td>\n",
       "      <td>Alabaster, Alabama</td>\n",
       "      <td>1179139816</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>C</td>\n",
       "      <td>5024356.0</td>\n",
       "      <td>South (East South Central)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20976</th>\n",
       "      <td>140221</td>\n",
       "      <td>Wamsutter, Wyoming</td>\n",
       "      <td>1169591845</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>GA</td>\n",
       "      <td>576837.0</td>\n",
       "      <td>West (Mountain)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20977</th>\n",
       "      <td>140185</td>\n",
       "      <td>Wheatland, Wyoming</td>\n",
       "      <td>1176370621</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>GA</td>\n",
       "      <td>576837.0</td>\n",
       "      <td>West (Mountain)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20978</th>\n",
       "      <td>140245</td>\n",
       "      <td>Worland, Wyoming</td>\n",
       "      <td>1166347917</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>GA</td>\n",
       "      <td>576837.0</td>\n",
       "      <td>West (Mountain)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20979</th>\n",
       "      <td>140070</td>\n",
       "      <td>Wright, Wyoming</td>\n",
       "      <td>1166334449</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>GA</td>\n",
       "      <td>576837.0</td>\n",
       "      <td>West (Mountain)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20980</th>\n",
       "      <td>140112</td>\n",
       "      <td>Yoder, Wyoming</td>\n",
       "      <td>1171182284</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>C</td>\n",
       "      <td>576837.0</td>\n",
       "      <td>West (Mountain)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20981 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pageid                title   lastrevid    state prediction  \\\n",
       "0      104730   Abbeville, Alabama  1171163550  Alabama          C   \n",
       "1      104761  Adamsville, Alabama  1177621427  Alabama          C   \n",
       "2      105188     Addison, Alabama  1168359898  Alabama          C   \n",
       "3      104726       Akron, Alabama  1165909508  Alabama         GA   \n",
       "4      105109   Alabaster, Alabama  1179139816  Alabama          C   \n",
       "...       ...                  ...         ...      ...        ...   \n",
       "20976  140221   Wamsutter, Wyoming  1169591845  Wyoming         GA   \n",
       "20977  140185   Wheatland, Wyoming  1176370621  Wyoming         GA   \n",
       "20978  140245     Worland, Wyoming  1166347917  Wyoming         GA   \n",
       "20979  140070      Wright, Wyoming  1166334449  Wyoming         GA   \n",
       "20980  140112       Yoder, Wyoming  1171182284  Wyoming          C   \n",
       "\n",
       "       April 1, 2020 Estimates Base           regional_devision  \n",
       "0                         5024356.0  South (East South Central)  \n",
       "1                         5024356.0  South (East South Central)  \n",
       "2                         5024356.0  South (East South Central)  \n",
       "3                         5024356.0  South (East South Central)  \n",
       "4                         5024356.0  South (East South Central)  \n",
       "...                             ...                         ...  \n",
       "20976                      576837.0             West (Mountain)  \n",
       "20977                      576837.0             West (Mountain)  \n",
       "20978                      576837.0             West (Mountain)  \n",
       "20979                      576837.0             West (Mountain)  \n",
       "20980                      576837.0             West (Mountain)  \n",
       "\n",
       "[20981 rows x 7 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8857d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results as per schema\n",
    "df_result.rename(columns={'April 1, 2020 Estimates Base':'population', \n",
    "                         'title': 'article_title',\n",
    "                         'lastrevid': 'revision_id',\n",
    "                         'prediction': 'article_quality'})[['state', 'regional_devision', 'population',\n",
    "                                                           'article_title', 'revision_id', 'article_quality']].to_csv('wp_scored_city_articles_by_state.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f4a651",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64cbd14",
   "metadata": {},
   "source": [
    "### Top 10 US states by coverage: The 10 US states with the highest total articles per capita (in descending order) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f1d7602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state\n",
       "Vermont          0.000512\n",
       "North_Dakota     0.000457\n",
       "Maine            0.000355\n",
       "South_Dakota     0.000351\n",
       "Iowa             0.000327\n",
       "Alaska           0.000203\n",
       "Pennsylvania     0.000197\n",
       "Michigan         0.000176\n",
       "Wyoming          0.000172\n",
       "New_Hampshire    0.000170\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_result.copy()\n",
    "df_top10 = df.groupby('state').count()['title']/df.groupby('state').min()['April 1, 2020 Estimates Base']\n",
    "\n",
    "df_top10.sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc66c6c",
   "metadata": {},
   "source": [
    "#### Bottom 10 US states by coverage: The 10 US states with the lowest total articles per capita (in ascending order) .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd0e9f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state\n",
       "North_Carolina    0.000005\n",
       "Nevada            0.000006\n",
       "California        0.000012\n",
       "Arizona           0.000013\n",
       "Virginia          0.000015\n",
       "Oklahoma          0.000019\n",
       "Florida           0.000019\n",
       "Kansas            0.000021\n",
       "Maryland          0.000025\n",
       "Wisconsin         0.000032\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_result.copy()\n",
    "df_bottom10 = df.groupby('state').count()['title']/df.groupby('state').min()['April 1, 2020 Estimates Base']\n",
    "\n",
    "df_bottom10.sort_values(ascending=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc8bffa",
   "metadata": {},
   "source": [
    "#### Top 10 US states by high quality: The 10 US states with the highest high quality articles per capita (in descending order) .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a90786d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state\n",
       "Vermont          0.000070\n",
       "Wyoming          0.000068\n",
       "South_Dakota     0.000063\n",
       "West_Virginia    0.000059\n",
       "Montana          0.000051\n",
       "New_Hampshire    0.000046\n",
       "Pennsylvania     0.000044\n",
       "Missouri         0.000043\n",
       "Alaska           0.000042\n",
       "New_Jersey       0.000041\n",
       "dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_result.copy()\n",
    "df['quality_count'] = df.prediction.apply(lambda x: 1 if x == 'FA' or  x == 'GA' else 0)\n",
    "df_top10 = df.groupby('state').sum()['quality_count']/df.groupby('state').min()['April 1, 2020 Estimates Base']\n",
    "\n",
    "df_top10.sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b380a53",
   "metadata": {},
   "source": [
    "#### Bottom 10 US states by high quality: The 10 US states with the lowest high quality articles per capita (in ascending order).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a3719e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state\n",
       "North_Carolina    0.000002\n",
       "Virginia          0.000002\n",
       "Nevada            0.000003\n",
       "Arizona           0.000003\n",
       "California        0.000004\n",
       "New_York          0.000005\n",
       "Florida           0.000006\n",
       "Maryland          0.000007\n",
       "Kansas            0.000007\n",
       "Oklahoma          0.000008\n",
       "dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_result.copy()\n",
    "df['quality_count'] = df.prediction.apply(lambda x: 1 if x == 'FA' or  x == 'GA' else 0)\n",
    "df_top10 = df.groupby('state').sum()['quality_count']/df.groupby('state').min()['April 1, 2020 Estimates Base']\n",
    "\n",
    "df_top10.sort_values(ascending=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338036dc",
   "metadata": {},
   "source": [
    "#### Census divisions by total coverage: A rank ordered list of US census divisions (in descending order) by total articles per capita.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30eafe59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "regional_devision\n",
       "Midwest (West North Central)    0.004591\n",
       "Northeast (New England)         0.002235\n",
       "West (Mountain)                 0.002061\n",
       "West (Pacific)                  0.001778\n",
       "South (South Atlantic)          0.001325\n",
       "Midwest (East North Central)    0.000806\n",
       "South (West South Central)      0.000697\n",
       "South (East South Central)      0.000516\n",
       "Northeast (Middle Atlantic)     0.000407\n",
       "dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_top10 = df.groupby('regional_devision').count()['title']/df.groupby('regional_devision').min()['April 1, 2020 Estimates Base']\n",
    "df_top10.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d92ae0f",
   "metadata": {},
   "source": [
    "#### Census divisions by high quality coverage: Rank ordered list of US census divisions (in descending order) by high quality articles per capita.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ffe7e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "regional_devision\n",
       "South (East South Central)      0.000107\n",
       "Northeast (Middle Atlantic)     0.000114\n",
       "Midwest (East North Central)    0.000122\n",
       "South (West South Central)      0.000210\n",
       "Northeast (New England)         0.000350\n",
       "South (South Atlantic)          0.000438\n",
       "West (Mountain)                 0.000581\n",
       "West (Pacific)                  0.000668\n",
       "Midwest (West North Central)    0.000820\n",
       "dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bottom10 = df.groupby('regional_devision').sum()['quality_count']/df.groupby('regional_devision').min()['April 1, 2020 Estimates Base']\n",
    "df_bottom10.sort_values(ascending=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
